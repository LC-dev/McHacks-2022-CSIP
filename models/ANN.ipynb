{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ANN.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# McHacks 2022 - COVID-19 Stringency Index Predictor"],"metadata":{"id":"emHo3Na5RQks"}},{"cell_type":"markdown","source":["# Imports"],"metadata":{"id":"ofMsO9vq7ZiQ"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import io\n","from sklearn import preprocessing\n","from sklearn.utils import shuffle\n","from google.colab import files"],"metadata":{"colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":178},"id":"0ERheeW9uASC","executionInfo":{"status":"ok","timestamp":1642926682106,"user_tz":300,"elapsed":51637,"user":{"displayName":"Abdullatif Hassan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01962299739210416728"}},"outputId":"fa0ec261-d271-4475-86da-8f7b8ea50f2c"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-4183a459-e52f-4c87-b45d-977cfda60836\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-4183a459-e52f-4c87-b45d-977cfda60836\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving McHacks_OCcovid.csv to McHacks_OCcovid.csv\n","Saving McHacks_EUcovid.csv to McHacks_EUcovid.csv\n","Saving McHacks_ALLcovid.csv to McHacks_ALLcovid.csv\n","Saving McHacks_NAcovid.csv to McHacks_NAcovid.csv\n"]}]},{"cell_type":"markdown","source":["# Data"],"metadata":{"id":"DvFdx4GaOpiG"}},{"cell_type":"code","source":["# Loading pre-processed data from csv\n","uploaded = files.upload()\n","\n","NA_frame = pd.read_csv(io.BytesIO(uploaded['McHacks_NAcovid.csv'])) # North America\n","OC_frame = pd.read_csv(io.BytesIO(uploaded['McHacks_OCcovid.csv'])) # Oceanea\n","EU_frame = pd.read_csv(io.BytesIO(uploaded['McHacks_EUcovid.csv'])) # Europe\n","All_frame = pd.read_csv(io.BytesIO(uploaded['McHacks_ALLcovid.csv'])) # All three continents combined\n","\n","# Useful metrics for the UI calibration\n","for name in All_frame.columns:\n","    max = All_frame[name].max()\n","    min = All_frame[name].min()\n","    median = All_frame[name].median()\n","    print(f'{name}:   min: {min}    max: {max}   median: {median}')\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MwPWpOfNcddM","executionInfo":{"status":"ok","timestamp":1642916612696,"user_tz":300,"elapsed":153,"user":{"displayName":"Abdullatif Hassan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01962299739210416728"}},"outputId":"2d0dbccb-7a1e-45ff-f046-08f0c1b197e8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["new_cases:   min: -74347    max: 1368382   median: 786.0\n","new_cases_smoothed:   min: -6223.0    max: 802223.429   median: 949.429\n","new_deaths:   min: -217    max: 4442   median: 7.0\n","new_deaths_smoothed:   min: -14.429    max: 3403.0   median: 8.286\n","reproduction_rate:   min: 0.19    max: 3.77   median: 1.05\n","icu_patients:   min: 0    max: 28891   median: 110.0\n","hosp_patients:   min: 0    max: 152032   median: 667.0\n","positive_rate:   min: 0.0    max: 0.5522   median: 0.0355\n","tests_per_case:   min: 0.0    max: 27035.0   median: 27.9\n","total_vaccinations:   min: 0    max: 530359800   median: 510049.0\n","people_vaccinated:   min: 0    max: 249586987   median: 356376.0\n","people_fully_vaccinated:   min: 0    max: 208808805   median: 114796.0\n","total_boosters:   min: 0    max: 81521247   median: 0.0\n","new_vaccinations_smoothed:   min: 0    max: 3508672   median: 5312.0\n","new_people_vaccinated_smoothed:   min: 0    max: 2028778   median: 1824.0\n","stringency_index:   min: 20.37    max: 100.0   median: 56.02\n","population:   min: 516100    max: 332915074   median: 8715494.0\n"]}]},{"cell_type":"code","source":["# Data processing functions\n","\n","def normalize_pop(csv):\n","  df = csv.copy()\n","\n","  to_be_normalized = ['new_cases','new_cases_smoothed', 'new_deaths', 'new_deaths_smoothed', 'icu_patients', 'hosp_patients', 'total_vaccinations', 'people_vaccinated',\n"," 'people_fully_vaccinated', 'total_boosters', 'new_vaccinations_smoothed',\n"," 'new_people_vaccinated_smoothed']\n","\n","  for i in to_be_normalized:\n","    df[i] = df[i]*1000000//df[\"population\"]\n","\n","  return df\n","\n","def normalize_features(df):\n","    result = df.copy()\n","\n","    for feature_name in df.columns:\n","        max_value = df[feature_name].max()\n","        min_value = df[feature_name].min()\n","\n","        if(max_value==min_value):\n","          max_value = min_value+1\n","\n","        result[feature_name] = (df[feature_name] - min_value) // (max_value - min_value)\n","\n","    return result\n","\n","def extract_xy(csv):\n","  csv = shuffle(csv)\n","  y = csv[\"stringency_index\"].to_numpy()\n","  csv = normalize_pop(csv)\n","  #csv = normalize_features(csv)\n","  x = csv.drop([\"stringency_index\"], axis=1).to_numpy()\n","  scaler = preprocessing.StandardScaler()\n","  scaler.fit(x)\n","  x = scaler.transform(x)\n","\n","  return x,y\n","\n","def split(x,y,low,high):\n","  x_train = np.concatenate((x[0:low], x[high:x.shape[0]]))\n","  y_train = np.concatenate((y[0:low], y[high:x.shape[0]]))\n","  x_test = x[low:high]\n","  y_test = y[low:high]\n","  return x_train, y_train, x_test, y_test\n","\n","\n","# Splitting data into features and labels\n","x_All, y_All = extract_xy(All_frame)\n","\n","# Uncomment if want to train only on specific continent\n","# x_OC, y_OC = extract_xy(OC_frame)\n","# x_EU, y_EU = extract_xy(EU_frame)\n"],"metadata":{"id":"cdPnt7ziunue"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Multi-layer Perceptron Regressor"],"metadata":{"id":"zg28SRXDhwyS"}},{"cell_type":"markdown","source":["#### Model definition"],"metadata":{"id":"6FCb282kTM1o"}},{"cell_type":"code","source":["def runMLP(X_train, y_train, X_test, y_test, momentum = 0.9):\n","  regr = MLPRegressor(random_state=1, max_iter=5000, momentum=momentum, verbose=True, tol=0.001, power_t=0.7).fit(X_train, y_train)\n","  y_pred = regr.predict(X_test)\n","  print(\"R Squared Error: \" + str(regr.score(X_test, y_test)))\n","  return y_pred"],"metadata":{"id":"nvdofZs10wjx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Training"],"metadata":{"id":"temHKsh6TBGb"}},{"cell_type":"code","source":["# Monitoring training for early stopping\n","# Randomly train and test data on last 25% of the data\n","\n","X_train, X_test, y_train, y_test = train_test_split(x_All, y_All)\n","y_pred = runMLP(X_train,y_train, X_test, y_test)\n","print(\"Momentum: \"+ str(m))\n","print(f'Y_Pred {np.round(y_pred, 2)}')\n","print(f'True OC{y_test}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k4WxM5jwPsPc","executionInfo":{"status":"ok","timestamp":1642911862669,"user_tz":300,"elapsed":48088,"user":{"displayName":"Abdullatif Hassan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01962299739210416728"}},"outputId":"65ed8ae7-a7a8-46f3-ae4a-03e6d6bcbc6e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration 1, loss = 1607.74132628\n","Iteration 2, loss = 1407.98324006\n","Iteration 3, loss = 1144.55547018\n","Iteration 4, loss = 841.13804756\n","Iteration 5, loss = 557.29212959\n","Iteration 6, loss = 352.90907659\n","Iteration 7, loss = 237.04174321\n","Iteration 8, loss = 185.64536510\n","Iteration 9, loss = 160.92431884\n","Iteration 10, loss = 145.91345680\n","Iteration 11, loss = 135.03674389\n","Iteration 12, loss = 126.99347546\n","Iteration 13, loss = 120.40830257\n","Iteration 14, loss = 115.17120774\n","Iteration 15, loss = 110.73626744\n","Iteration 16, loss = 106.75638494\n","Iteration 17, loss = 103.44174640\n","Iteration 18, loss = 100.80428508\n","Iteration 19, loss = 98.15451521\n","Iteration 20, loss = 96.18225376\n","Iteration 21, loss = 94.26855968\n","Iteration 22, loss = 92.78851396\n","Iteration 23, loss = 91.63889357\n","Iteration 24, loss = 90.28883550\n","Iteration 25, loss = 89.61369471\n","Iteration 26, loss = 87.89802322\n","Iteration 27, loss = 86.81367936\n","Iteration 28, loss = 85.69023368\n","Iteration 29, loss = 84.72655956\n","Iteration 30, loss = 83.80671219\n","Iteration 31, loss = 82.94726630\n","Iteration 32, loss = 82.09378753\n","Iteration 33, loss = 81.31702172\n","Iteration 34, loss = 80.54158626\n","Iteration 35, loss = 79.93812710\n","Iteration 36, loss = 79.22544111\n","Iteration 37, loss = 79.08029039\n","Iteration 38, loss = 77.95758797\n","Iteration 39, loss = 77.07248332\n","Iteration 40, loss = 76.67510932\n","Iteration 41, loss = 75.91288162\n","Iteration 42, loss = 75.46010039\n","Iteration 43, loss = 75.22990804\n","Iteration 44, loss = 74.40789945\n","Iteration 45, loss = 74.05554552\n","Iteration 46, loss = 73.48479900\n","Iteration 47, loss = 73.07833231\n","Iteration 48, loss = 72.64219586\n","Iteration 49, loss = 72.29812473\n","Iteration 50, loss = 71.89073575\n","Iteration 51, loss = 72.21927952\n","Iteration 52, loss = 71.26649122\n","Iteration 53, loss = 70.82440118\n","Iteration 54, loss = 70.74925184\n","Iteration 55, loss = 70.59224707\n","Iteration 56, loss = 69.83461398\n","Iteration 57, loss = 69.87711818\n","Iteration 58, loss = 70.35501767\n","Iteration 59, loss = 69.09312285\n","Iteration 60, loss = 69.03788148\n","Iteration 61, loss = 68.48340976\n","Iteration 62, loss = 68.05113573\n","Iteration 63, loss = 67.74140499\n","Iteration 64, loss = 67.88721125\n","Iteration 65, loss = 67.24693077\n","Iteration 66, loss = 67.22039483\n","Iteration 67, loss = 66.75641768\n","Iteration 68, loss = 66.88480973\n","Iteration 69, loss = 66.31303071\n","Iteration 70, loss = 68.78424627\n","Iteration 71, loss = 66.19214733\n","Iteration 72, loss = 65.79026886\n","Iteration 73, loss = 65.50664996\n","Iteration 74, loss = 65.60946246\n","Iteration 75, loss = 65.05163758\n","Iteration 76, loss = 64.89074114\n","Iteration 77, loss = 64.74511844\n","Iteration 78, loss = 64.57588638\n","Iteration 79, loss = 64.46915799\n","Iteration 80, loss = 64.75742098\n","Iteration 81, loss = 64.24163402\n","Iteration 82, loss = 64.11258336\n","Iteration 83, loss = 63.66729098\n","Iteration 84, loss = 63.44514766\n","Iteration 85, loss = 63.37157005\n","Iteration 86, loss = 63.42212826\n","Iteration 87, loss = 63.00090319\n","Iteration 88, loss = 62.93626203\n","Iteration 89, loss = 62.76073449\n","Iteration 90, loss = 62.75458290\n","Iteration 91, loss = 62.76828608\n","Iteration 92, loss = 62.24760999\n","Iteration 93, loss = 62.19810425\n","Iteration 94, loss = 61.95319262\n","Iteration 95, loss = 61.82504545\n","Iteration 96, loss = 62.05268296\n","Iteration 97, loss = 62.06748624\n","Iteration 98, loss = 61.70250218\n","Iteration 99, loss = 61.32028261\n","Iteration 100, loss = 61.21033955\n","Iteration 101, loss = 61.15900246\n","Iteration 102, loss = 61.26413326\n","Iteration 103, loss = 60.80322361\n","Iteration 104, loss = 60.93265934\n","Iteration 105, loss = 60.56749557\n","Iteration 106, loss = 60.50320352\n","Iteration 107, loss = 60.34839561\n","Iteration 108, loss = 60.25583311\n","Iteration 109, loss = 60.18177087\n","Iteration 110, loss = 60.05947476\n","Iteration 111, loss = 59.97649066\n","Iteration 112, loss = 60.23110277\n","Iteration 113, loss = 59.72854603\n","Iteration 114, loss = 59.68512590\n","Iteration 115, loss = 59.51656638\n","Iteration 116, loss = 59.40768172\n","Iteration 117, loss = 59.35273153\n","Iteration 118, loss = 59.40639369\n","Iteration 119, loss = 59.11581586\n","Iteration 120, loss = 59.39985652\n","Iteration 121, loss = 58.93517407\n","Iteration 122, loss = 58.79571908\n","Iteration 123, loss = 59.37808794\n","Iteration 124, loss = 58.79004060\n","Iteration 125, loss = 58.51380200\n","Iteration 126, loss = 58.62956584\n","Iteration 127, loss = 58.67968134\n","Iteration 128, loss = 58.31332233\n","Iteration 129, loss = 58.32929660\n","Iteration 130, loss = 58.89307788\n","Iteration 131, loss = 58.01334405\n","Iteration 132, loss = 58.79709826\n","Iteration 133, loss = 57.79851003\n","Iteration 134, loss = 57.66721654\n","Iteration 135, loss = 57.91319588\n","Iteration 136, loss = 57.47233123\n","Iteration 137, loss = 57.38210555\n","Iteration 138, loss = 57.44342372\n","Iteration 139, loss = 57.36649660\n","Iteration 140, loss = 57.15917174\n","Iteration 141, loss = 57.10876523\n","Iteration 142, loss = 57.34556960\n","Iteration 143, loss = 56.96651307\n","Iteration 144, loss = 57.01813734\n","Iteration 145, loss = 56.75846648\n","Iteration 146, loss = 56.67984790\n","Iteration 147, loss = 57.09500928\n","Iteration 148, loss = 56.78466244\n","Iteration 149, loss = 56.51908789\n","Iteration 150, loss = 56.42683640\n","Iteration 151, loss = 56.62064318\n","Iteration 152, loss = 56.38657048\n","Iteration 153, loss = 56.32325511\n","Iteration 154, loss = 56.13162112\n","Iteration 155, loss = 56.27453141\n","Iteration 156, loss = 55.99153290\n","Iteration 157, loss = 57.21350764\n","Iteration 158, loss = 56.15297940\n","Iteration 159, loss = 56.20776204\n","Iteration 160, loss = 55.70330605\n","Iteration 161, loss = 55.59950826\n","Iteration 162, loss = 55.52520037\n","Iteration 163, loss = 55.51840004\n","Iteration 164, loss = 56.91444007\n","Iteration 165, loss = 55.87710679\n","Iteration 166, loss = 55.56915373\n","Iteration 167, loss = 55.24818131\n","Iteration 168, loss = 55.17011146\n","Iteration 169, loss = 55.43027487\n","Iteration 170, loss = 55.16683211\n","Iteration 171, loss = 55.07961301\n","Iteration 172, loss = 55.00705071\n","Iteration 173, loss = 54.94622072\n","Iteration 174, loss = 55.00322041\n","Iteration 175, loss = 54.80156870\n","Iteration 176, loss = 54.74191946\n","Iteration 177, loss = 54.67981305\n","Iteration 178, loss = 55.05629383\n","Iteration 179, loss = 54.64840761\n","Iteration 180, loss = 54.93344040\n","Iteration 181, loss = 54.56363238\n","Iteration 182, loss = 54.43226249\n","Iteration 183, loss = 54.47471684\n","Iteration 184, loss = 54.75692843\n","Iteration 185, loss = 54.88547566\n","Iteration 186, loss = 54.35800073\n","Iteration 187, loss = 54.64595919\n","Iteration 188, loss = 54.23758215\n","Iteration 189, loss = 54.16251799\n","Iteration 190, loss = 53.96978782\n","Iteration 191, loss = 53.91870458\n","Iteration 192, loss = 53.90376765\n","Iteration 193, loss = 54.00215069\n","Iteration 194, loss = 54.10099315\n","Iteration 195, loss = 54.23408016\n","Iteration 196, loss = 53.70557963\n","Iteration 197, loss = 53.87039586\n","Iteration 198, loss = 53.61410219\n","Iteration 199, loss = 54.13078082\n","Iteration 200, loss = 53.49677822\n","Iteration 201, loss = 53.49355928\n","Iteration 202, loss = 53.39808099\n","Iteration 203, loss = 53.43716793\n","Iteration 204, loss = 53.22267516\n","Iteration 205, loss = 53.41726370\n","Iteration 206, loss = 53.54889814\n","Iteration 207, loss = 53.96175054\n","Iteration 208, loss = 53.07829007\n","Iteration 209, loss = 53.01573563\n","Iteration 210, loss = 53.31231206\n","Iteration 211, loss = 52.96905612\n","Iteration 212, loss = 53.40128215\n","Iteration 213, loss = 53.53369688\n","Iteration 214, loss = 52.92846923\n","Iteration 215, loss = 52.83484411\n","Iteration 216, loss = 52.69573318\n","Iteration 217, loss = 52.63562979\n","Iteration 218, loss = 52.80539657\n","Iteration 219, loss = 52.97437205\n","Iteration 220, loss = 52.53463551\n","Iteration 221, loss = 52.41322779\n","Iteration 222, loss = 52.42214321\n","Iteration 223, loss = 52.50582300\n","Iteration 224, loss = 52.28365892\n","Iteration 225, loss = 52.19950998\n","Iteration 226, loss = 52.65924126\n","Iteration 227, loss = 52.21034838\n","Iteration 228, loss = 52.38599413\n","Iteration 229, loss = 52.44482436\n","Iteration 230, loss = 52.68524340\n","Iteration 231, loss = 52.03598649\n","Iteration 232, loss = 52.03257097\n","Iteration 233, loss = 51.95058563\n","Iteration 234, loss = 52.07336347\n","Iteration 235, loss = 52.04184144\n","Iteration 236, loss = 51.71688291\n","Iteration 237, loss = 52.42748767\n","Iteration 238, loss = 51.97543451\n","Iteration 239, loss = 53.03467289\n","Iteration 240, loss = 51.85221755\n","Iteration 241, loss = 51.55974792\n","Iteration 242, loss = 51.47172823\n","Iteration 243, loss = 52.56655655\n","Iteration 244, loss = 51.53034013\n","Iteration 245, loss = 51.94045726\n","Iteration 246, loss = 51.57483567\n","Iteration 247, loss = 51.28996693\n","Iteration 248, loss = 51.63762947\n","Iteration 249, loss = 52.10742923\n","Iteration 250, loss = 51.26067283\n","Iteration 251, loss = 51.21593377\n","Iteration 252, loss = 51.27820980\n","Iteration 253, loss = 51.27481972\n","Iteration 254, loss = 50.96602806\n","Iteration 255, loss = 50.91760679\n","Iteration 256, loss = 51.17665548\n","Iteration 257, loss = 51.07554401\n","Iteration 258, loss = 50.96515724\n","Iteration 259, loss = 50.93818929\n","Iteration 260, loss = 50.72460074\n","Iteration 261, loss = 50.63973837\n","Iteration 262, loss = 50.57464198\n","Iteration 263, loss = 50.51504727\n","Iteration 264, loss = 50.50992121\n","Iteration 265, loss = 50.46156621\n","Iteration 266, loss = 50.42809711\n","Iteration 267, loss = 51.26117156\n","Iteration 268, loss = 50.39387398\n","Iteration 269, loss = 50.29133148\n","Iteration 270, loss = 50.27359923\n","Iteration 271, loss = 50.64703443\n","Iteration 272, loss = 50.20340668\n","Iteration 273, loss = 50.10374610\n","Iteration 274, loss = 50.27701696\n","Iteration 275, loss = 50.03902313\n","Iteration 276, loss = 50.08565478\n","Iteration 277, loss = 50.13461863\n","Iteration 278, loss = 49.99776839\n","Iteration 279, loss = 49.90210454\n","Iteration 280, loss = 49.97200866\n","Iteration 281, loss = 50.03480566\n","Iteration 282, loss = 49.96684541\n","Iteration 283, loss = 49.69657403\n","Iteration 284, loss = 51.91341486\n","Iteration 285, loss = 50.36209014\n","Iteration 286, loss = 49.78204405\n","Iteration 287, loss = 49.60836645\n","Iteration 288, loss = 49.52037415\n","Iteration 289, loss = 49.46639191\n","Iteration 290, loss = 49.49062534\n","Iteration 291, loss = 49.43194385\n","Iteration 292, loss = 49.32832366\n","Iteration 293, loss = 49.57950118\n","Iteration 294, loss = 49.44374464\n","Iteration 295, loss = 49.41262583\n","Iteration 296, loss = 49.31864604\n","Iteration 297, loss = 49.23607071\n","Iteration 298, loss = 49.47049311\n","Iteration 299, loss = 49.03179353\n","Iteration 300, loss = 48.99041452\n","Iteration 301, loss = 49.01928792\n","Iteration 302, loss = 49.10889504\n","Iteration 303, loss = 48.92125999\n","Iteration 304, loss = 48.83177896\n","Iteration 305, loss = 48.84149130\n","Iteration 306, loss = 48.70875835\n","Iteration 307, loss = 48.72560157\n","Iteration 308, loss = 48.66483201\n","Iteration 309, loss = 49.42344738\n","Iteration 310, loss = 48.66639843\n","Iteration 311, loss = 48.55525585\n","Iteration 312, loss = 48.45077128\n","Iteration 313, loss = 49.82555359\n","Iteration 314, loss = 48.88061765\n","Iteration 315, loss = 48.42592879\n","Iteration 316, loss = 49.86006699\n","Iteration 317, loss = 48.55487007\n","Iteration 318, loss = 48.38166556\n","Iteration 319, loss = 48.55256664\n","Iteration 320, loss = 48.30122741\n","Iteration 321, loss = 48.42063250\n","Iteration 322, loss = 48.16981694\n","Iteration 323, loss = 48.12379737\n","Iteration 324, loss = 48.83914590\n","Iteration 325, loss = 48.05661939\n","Iteration 326, loss = 48.14746038\n","Iteration 327, loss = 48.07264832\n","Iteration 328, loss = 47.89754016\n","Iteration 329, loss = 48.25472322\n","Iteration 330, loss = 49.29621632\n","Iteration 331, loss = 47.81595195\n","Iteration 332, loss = 48.02952770\n","Iteration 333, loss = 48.16754202\n","Iteration 334, loss = 47.84731406\n","Iteration 335, loss = 47.90215642\n","Iteration 336, loss = 47.72426424\n","Iteration 337, loss = 47.65028376\n","Iteration 338, loss = 47.93395674\n","Iteration 339, loss = 47.65835829\n","Iteration 340, loss = 47.54458294\n","Iteration 341, loss = 47.48181201\n","Iteration 342, loss = 47.54910130\n","Iteration 343, loss = 47.43854577\n","Iteration 344, loss = 47.49531644\n","Iteration 345, loss = 48.53402680\n","Iteration 346, loss = 47.38991533\n","Iteration 347, loss = 47.32869177\n","Iteration 348, loss = 47.31322360\n","Iteration 349, loss = 47.40324084\n","Iteration 350, loss = 47.38262343\n","Iteration 351, loss = 47.57439830\n","Iteration 352, loss = 47.98227657\n","Iteration 353, loss = 47.12108248\n","Iteration 354, loss = 47.11050775\n","Iteration 355, loss = 46.98178654\n","Iteration 356, loss = 46.94732744\n","Iteration 357, loss = 47.04749413\n","Iteration 358, loss = 47.31312636\n","Iteration 359, loss = 47.70660429\n","Iteration 360, loss = 46.91175452\n","Iteration 361, loss = 47.14392542\n","Iteration 362, loss = 46.79586444\n","Iteration 363, loss = 46.76536580\n","Iteration 364, loss = 46.76706618\n","Iteration 365, loss = 46.79285751\n","Iteration 366, loss = 46.74125070\n","Iteration 367, loss = 46.62811168\n","Iteration 368, loss = 46.61311843\n","Iteration 369, loss = 46.64295943\n","Iteration 370, loss = 46.54559052\n","Iteration 371, loss = 46.60784848\n","Iteration 372, loss = 46.92862773\n","Iteration 373, loss = 46.71144569\n","Iteration 374, loss = 46.49173882\n","Iteration 375, loss = 47.70529790\n","Iteration 376, loss = 46.54102066\n","Iteration 377, loss = 46.36404534\n","Iteration 378, loss = 46.29971572\n","Iteration 379, loss = 46.68378924\n","Iteration 380, loss = 46.58671340\n","Iteration 381, loss = 46.73195495\n","Iteration 382, loss = 46.18046957\n","Iteration 383, loss = 46.71105292\n","Iteration 384, loss = 46.13082911\n","Iteration 385, loss = 46.03559885\n","Iteration 386, loss = 46.01721095\n","Iteration 387, loss = 46.35365602\n","Iteration 388, loss = 46.13101373\n","Iteration 389, loss = 46.00315098\n","Iteration 390, loss = 45.92455938\n","Iteration 391, loss = 45.87118380\n","Iteration 392, loss = 45.90464610\n","Iteration 393, loss = 46.84970832\n","Iteration 394, loss = 45.89651554\n","Iteration 395, loss = 45.80549065\n","Iteration 396, loss = 45.93150133\n","Iteration 397, loss = 45.81070983\n","Iteration 398, loss = 45.66039315\n","Iteration 399, loss = 46.05561374\n","Iteration 400, loss = 45.70062533\n","Iteration 401, loss = 45.57688090\n","Iteration 402, loss = 45.54119001\n","Iteration 403, loss = 45.53386160\n","Iteration 404, loss = 45.43040502\n","Iteration 405, loss = 45.46828489\n","Iteration 406, loss = 47.42416098\n","Iteration 407, loss = 45.42170676\n","Iteration 408, loss = 45.42165074\n","Iteration 409, loss = 45.32945028\n","Iteration 410, loss = 45.31808181\n","Iteration 411, loss = 45.30536751\n","Iteration 412, loss = 45.25384393\n","Iteration 413, loss = 45.31405153\n","Iteration 414, loss = 45.71775172\n","Iteration 415, loss = 45.32867581\n","Iteration 416, loss = 45.19936868\n","Iteration 417, loss = 45.54864656\n","Iteration 418, loss = 45.37228250\n","Iteration 419, loss = 45.07902361\n","Iteration 420, loss = 45.06736989\n","Iteration 421, loss = 45.44495935\n","Iteration 422, loss = 44.96975386\n","Iteration 423, loss = 45.18670497\n","Iteration 424, loss = 45.52680381\n","Iteration 425, loss = 45.01103542\n","Iteration 426, loss = 45.06898464\n","Iteration 427, loss = 44.82719027\n","Iteration 428, loss = 44.86723385\n","Iteration 429, loss = 44.79427687\n","Iteration 430, loss = 45.11732933\n","Iteration 431, loss = 44.68888192\n","Iteration 432, loss = 45.50595918\n","Iteration 433, loss = 45.32059043\n","Iteration 434, loss = 44.67353916\n","Iteration 435, loss = 45.00595747\n","Iteration 436, loss = 44.73866008\n","Iteration 437, loss = 45.73295890\n","Iteration 438, loss = 44.67790529\n","Iteration 439, loss = 44.63233561\n","Iteration 440, loss = 44.67482507\n","Iteration 441, loss = 44.88581972\n","Iteration 442, loss = 44.78967668\n","Iteration 443, loss = 44.93285530\n","Iteration 444, loss = 44.51938039\n","Iteration 445, loss = 44.39742204\n","Iteration 446, loss = 44.27494551\n","Iteration 447, loss = 44.52075481\n","Iteration 448, loss = 44.38532251\n","Iteration 449, loss = 44.24881769\n","Iteration 450, loss = 44.34407765\n","Iteration 451, loss = 44.79271280\n","Iteration 452, loss = 44.19637623\n","Iteration 453, loss = 44.48099948\n","Iteration 454, loss = 44.17516905\n","Iteration 455, loss = 44.14577415\n","Iteration 456, loss = 44.05557742\n","Iteration 457, loss = 43.95495233\n","Iteration 458, loss = 44.71281236\n","Iteration 459, loss = 44.07306045\n","Iteration 460, loss = 43.95253827\n","Iteration 461, loss = 43.98713413\n","Iteration 462, loss = 43.88050626\n","Iteration 463, loss = 45.16316277\n","Iteration 464, loss = 43.99892489\n","Iteration 465, loss = 43.86012375\n","Iteration 466, loss = 43.92447155\n","Iteration 467, loss = 44.28055237\n","Iteration 468, loss = 43.79736795\n","Iteration 469, loss = 43.71062403\n","Iteration 470, loss = 43.92312722\n","Iteration 471, loss = 43.64504133\n","Iteration 472, loss = 43.63392872\n","Iteration 473, loss = 44.48236455\n","Iteration 474, loss = 43.72659658\n","Iteration 475, loss = 44.23369154\n","Iteration 476, loss = 43.63938972\n","Iteration 477, loss = 43.59809240\n","Iteration 478, loss = 44.04299604\n","Iteration 479, loss = 43.91208318\n","Iteration 480, loss = 43.48149461\n","Iteration 481, loss = 44.13866989\n","Iteration 482, loss = 43.57548770\n","Iteration 483, loss = 43.80296533\n","Iteration 484, loss = 43.79432436\n","Iteration 485, loss = 43.55493284\n","Iteration 486, loss = 43.40117600\n","Iteration 487, loss = 43.46408044\n","Iteration 488, loss = 43.37712171\n","Iteration 489, loss = 43.65210229\n","Iteration 490, loss = 43.46144702\n","Iteration 491, loss = 43.20127219\n","Iteration 492, loss = 43.60853559\n","Iteration 493, loss = 43.22566350\n","Iteration 494, loss = 43.18425633\n","Iteration 495, loss = 43.02398391\n","Iteration 496, loss = 43.05782102\n","Iteration 497, loss = 43.79805209\n","Iteration 498, loss = 43.23188623\n","Iteration 499, loss = 43.00993445\n","Iteration 500, loss = 43.09201126\n","Iteration 501, loss = 44.14538506\n","Iteration 502, loss = 44.33573314\n","Iteration 503, loss = 43.27392503\n","Iteration 504, loss = 42.89964973\n","Iteration 505, loss = 42.87293865\n","Iteration 506, loss = 42.78345221\n","Iteration 507, loss = 43.14157034\n","Iteration 508, loss = 42.95028877\n","Iteration 509, loss = 42.87907229\n","Iteration 510, loss = 43.66941275\n","Iteration 511, loss = 42.97699327\n","Iteration 512, loss = 42.82314451\n","Iteration 513, loss = 43.53220501\n","Iteration 514, loss = 42.76035132\n","Iteration 515, loss = 42.63364435\n","Iteration 516, loss = 42.75465615\n","Iteration 517, loss = 42.58526559\n","Iteration 518, loss = 42.57953171\n","Iteration 519, loss = 43.14261245\n","Iteration 520, loss = 42.67096910\n","Iteration 521, loss = 42.51198044\n","Iteration 522, loss = 42.85892878\n","Iteration 523, loss = 42.79386537\n","Iteration 524, loss = 42.52305284\n","Iteration 525, loss = 42.54998855\n","Iteration 526, loss = 42.53101700\n","Iteration 527, loss = 42.35099106\n","Iteration 528, loss = 42.44221431\n","Iteration 529, loss = 42.73263808\n","Iteration 530, loss = 42.36355012\n","Iteration 531, loss = 42.47248161\n","Iteration 532, loss = 42.42085699\n","Iteration 533, loss = 42.22774770\n","Iteration 534, loss = 42.26569998\n","Iteration 535, loss = 42.61167498\n","Iteration 536, loss = 42.20311978\n","Iteration 537, loss = 43.68740743\n","Iteration 538, loss = 42.44803321\n","Iteration 539, loss = 42.20038913\n","Iteration 540, loss = 42.07182203\n","Iteration 541, loss = 42.04665496\n","Iteration 542, loss = 42.08940062\n","Iteration 543, loss = 41.99131148\n","Iteration 544, loss = 41.92557825\n","Iteration 545, loss = 42.76408054\n","Iteration 546, loss = 42.07273705\n","Iteration 547, loss = 42.30562215\n","Iteration 548, loss = 42.22170170\n","Iteration 549, loss = 42.47064300\n","Iteration 550, loss = 41.90863946\n","Iteration 551, loss = 41.89641775\n","Iteration 552, loss = 41.86451624\n","Iteration 553, loss = 41.86497943\n","Iteration 554, loss = 41.93898614\n","Iteration 555, loss = 41.69676304\n","Iteration 556, loss = 42.70702908\n","Iteration 557, loss = 42.51346445\n","Iteration 558, loss = 43.31768605\n","Iteration 559, loss = 41.86351500\n","Iteration 560, loss = 41.72605201\n","Iteration 561, loss = 41.75096491\n","Iteration 562, loss = 41.64400513\n","Iteration 563, loss = 41.88801278\n","Iteration 564, loss = 41.55020783\n","Iteration 565, loss = 41.62694319\n","Iteration 566, loss = 41.55785965\n","Iteration 567, loss = 41.49252162\n","Iteration 568, loss = 41.49765956\n","Iteration 569, loss = 41.68653371\n","Iteration 570, loss = 41.58691862\n","Iteration 571, loss = 41.82555349\n","Iteration 572, loss = 41.42988382\n","Iteration 573, loss = 41.52870553\n","Iteration 574, loss = 41.40043948\n","Iteration 575, loss = 41.33322513\n","Iteration 576, loss = 41.57120401\n","Iteration 577, loss = 41.29432816\n","Iteration 578, loss = 41.40630675\n","Iteration 579, loss = 41.49950558\n","Iteration 580, loss = 41.30601743\n","Iteration 581, loss = 42.27251878\n","Iteration 582, loss = 41.29625305\n","Iteration 583, loss = 41.25373047\n","Iteration 584, loss = 41.60173208\n","Iteration 585, loss = 41.16735959\n","Iteration 586, loss = 41.28035956\n","Iteration 587, loss = 41.80222335\n","Iteration 588, loss = 41.45929405\n","Iteration 589, loss = 41.05606449\n","Iteration 590, loss = 41.07658009\n","Iteration 591, loss = 41.41114589\n","Iteration 592, loss = 41.18300803\n","Iteration 593, loss = 41.04727100\n","Iteration 594, loss = 41.91866835\n","Iteration 595, loss = 41.03637969\n","Iteration 596, loss = 41.02205819\n","Iteration 597, loss = 40.92904498\n","Iteration 598, loss = 40.95958738\n","Iteration 599, loss = 41.55803838\n","Iteration 600, loss = 40.98634663\n","Iteration 601, loss = 40.82881366\n","Iteration 602, loss = 42.66060271\n","Iteration 603, loss = 40.99214829\n","Iteration 604, loss = 40.83798104\n","Iteration 605, loss = 40.82410746\n","Iteration 606, loss = 40.80395764\n","Iteration 607, loss = 40.68244581\n","Iteration 608, loss = 40.86558143\n","Iteration 609, loss = 40.93576010\n","Iteration 610, loss = 40.77087997\n","Iteration 611, loss = 40.79185447\n","Iteration 612, loss = 40.71556794\n","Iteration 613, loss = 42.23422054\n","Iteration 614, loss = 41.11923229\n","Iteration 615, loss = 40.69450895\n","Iteration 616, loss = 40.70178598\n","Iteration 617, loss = 40.68553297\n","Iteration 618, loss = 40.77607478\n","Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n","R Squared Error: 0.6556729840775057\n","Momentum: 0.9\n","Y_Pred [47.72 79.49 55.98 ... 60.01 72.12 59.3 ]\n","True OC[41.2  79.63 53.7  ... 69.91 85.19 62.5 ]\n"]}]},{"cell_type":"code","source":["# Randomly splitting train and test data\n","X_train, y_train, X_test, y_test = split(x_All,y_All,600, 800)\n","y_pred = runMLP(X_train, y_train, X_test, y_test)\n","print(f'Y_Pred {np.round(y_pred, 2)}')\n","print(f'True OC{y_test}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VIOuGAgkdotA","executionInfo":{"status":"ok","timestamp":1642910438834,"user_tz":300,"elapsed":122064,"user":{"displayName":"Abdullatif Hassan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01962299739210416728"}},"outputId":"0030d386-d22a-4410-a588-7ebd6729f059"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["R Squared Error: 0.7260559184819968\n","Y_Pred [62.74 68.13 68.68 52.81 46.44 48.02 73.58 51.63 85.56 57.89 72.95 72.04\n"," 68.35 51.99 50.71 65.53 48.99 79.01 67.89 57.33 44.11 34.36 44.66 65.21\n"," 71.09 62.48 66.   26.34 45.32 43.15 79.33 49.94 57.15 49.4  87.14 43.7\n"," 29.32 54.79 73.18 51.1  55.91 45.06 71.54 65.6  40.75 56.95 66.16 25.9\n"," 48.07 58.16 65.   57.57 48.4  75.08 60.47 80.63 47.69 32.96 60.77 60.17\n"," 55.28 50.44 42.99 57.87 78.76 65.29 45.97 41.19 70.87 65.7  68.1  51.06\n"," 81.74 52.88 77.03 58.23 59.63 53.18 73.56 41.26 70.71 65.57 54.07 64.38\n"," 51.74 34.03 63.24 46.52 42.48 54.22 61.1  44.23 62.19 65.96 80.87 57.4\n"," 66.22 47.36 70.3  60.83 41.86 53.57 35.83 52.23 48.91 47.57 64.78 52.57\n"," 48.56 37.16 60.58 49.85 51.31 70.53 36.65 42.41 64.55 39.35 62.93 60.69\n"," 88.12 73.09 33.22 74.17 41.75 63.33 54.77 61.78 52.63 54.19 61.98 44.43\n"," 52.38 59.94 60.76 63.52 58.52 43.94 81.38 31.12 53.99 47.26 69.86 38.4\n"," 42.77 56.9  36.69 54.34 41.36 66.78 55.32 28.02 54.06 91.11 68.08 60.44\n"," 47.34 61.75 76.56 47.61 72.42 68.89 82.42 61.87 56.32 70.98 81.72 49.57\n"," 46.92 29.   44.53 73.2  44.79 48.95 50.25 64.69 30.64 64.39 51.61 54.4\n"," 71.4  64.68 76.12 61.51 62.09 50.44 56.85 50.06 65.04 54.18 58.2  66.44\n"," 63.59 72.16 69.63 64.88 43.72 77.44 42.75 64.74]\n","True OC[61.57 75.46 62.96 46.76 39.81 37.96 75.46 52.31 93.52 22.22 67.59 74.07\n"," 72.22 56.02 47.22 59.26 50.93 85.19 65.74 50.93 43.52 41.2  43.98 67.59\n"," 75.46 66.67 73.15 20.37 41.67 31.48 76.85 52.78 50.   60.65 86.11 46.3\n"," 22.22 78.7  79.63 58.33 43.52 43.98 71.3  64.35 44.44 65.74 66.67 22.22\n"," 47.22 38.89 73.15 55.56 50.   74.07 56.48 81.48 43.06 22.22 66.2  68.06\n"," 34.72 47.69 41.67 51.85 78.7  92.59 27.78 44.44 73.15 71.76 55.56 36.11\n"," 75.93 56.02 79.63 52.78 60.19 29.17 69.44 47.22 63.89 67.59 37.04 70.37\n"," 50.   42.59 62.5  57.41 43.52 55.56 81.02 45.83 62.04 64.35 96.3  72.22\n"," 62.96 37.96 71.3  63.43 43.06 38.89 38.89 43.06 35.65 67.13 62.04 58.33\n"," 57.41 29.63 59.26 50.   50.46 68.06 27.78 45.83 64.35 24.07 62.5  56.48\n"," 96.3  68.52 35.19 75.46 43.98 69.91 39.81 63.89 51.85 55.56 75.   50.\n"," 42.59 40.28 61.11 60.19 38.89 32.41 87.96 22.22 54.63 40.74 69.44 38.89\n"," 54.63 62.96 29.63 75.46 32.41 61.11 56.94 20.37 47.22 96.3  75.46 61.11\n"," 64.35 62.5  90.74 50.   71.3  75.93 82.41 62.04 67.59 72.22 81.48 51.85\n"," 41.2  28.7  35.19 75.   48.15 73.15 43.98 78.24 32.41 56.48 47.69 52.78\n"," 70.37 68.52 73.15 60.19 81.48 72.22 56.48 44.91 64.81 50.93 38.89 69.91\n"," 80.56 84.26 67.59 70.37 50.93 69.44 41.67 71.3 ]\n"]}]}]}
